# kafka

데이터 전송 라인이 많아지면 배포와 장애에 대응하기 어려움

소스 어플리케이션 & 타겟 어플리케이션 연결 느슨하게 하기 위해 등장

소스 어플리케이션은 카프카에 데이터 전송 (쇼핑몰의 클릭, 결제 로그 저장, 데이터 포맷 제한 거의 없음)

타겟 어플리케이션은 카프카에서 데이터 가져옴 (로그 적재, 로그 처리)

카프카는 각종 데이터를 담는 Topic이라는 개념 있음 (큐와 비슷)

카프카 프로듀서(소스 어플리케이션)

카프카 컨슈머(타겟 어플리케이션)

⇒ 프로듀서, 컨슈머 각각 라이브러리로 개발되어 있어서 어플리케이션으로 구현 가능

카프카는 낮은 지연, 높은 처리량으로 데이터 처리 효과적으로 가능

### 토픽

토픽 여러 개 생성 가능

데이터베이스의 테이블이나 파일시스템의 폴더와 유사한 성질

이 토픽에 프로듀서가 데이터를 넣고, 컨슈머는 데이터를 가져감

각 토픽은 이름 설정 가능하고, 어떤 용도로 사용되는지 명확한 이름 짓기(click_log, send_sms 등)

하나의 토픽 여러 개의 파티션으로 구성 가능

파티션은 0번부터 시작

파티션은 큐처럼 끝에서부터 데이터가 차곡차곡 쌓이고 컨슈머가 가장 오래된 순서부터 가져감 (컨슈머가 데이터 가져가더라도 파티션 내부 데이터는 삭제되지 않음)

새로운 컨슈머가 붙으면 0번부터 그 컨슈머가 가져가서 사용 가능 (제약조건 : 컨슈머 그룹이 달라야 함, `auto.offset.reset = earliest` 여야 함)

더 이상 데이터가 들어오지 않으면 컨슈머는 또 다른 데이터가 들어올 때까지 기다림

파티션 2개 이상인 경우

파티션에 데이터를 보낼 때 키를 지정할 수 있음

1. 키가 null이고, 기본 파티셔너 사용 → 라운드 로빈으로 할당
2. 키가 있고, 기본 파티셔너 사용 → 키의 해시값 구하고, 특정 파티션에 할당

### 카프카 브로커

카프카가 설치되어 있는 서버 단위

3개 이상의 브로커로 구성해서 사용하는 것 권장

파티션 1개, 레플리케이션 1인 토픽 존재, 브로커 3대 → 브로커 3대 중 1대에 해당 토픽 정보 저장됨

레플리케이션 : 파티션 복제 (레플리케이션 1 : 파티션 1개 존재, 레플리케이션 2 : 파티션 1개, 복제본 1개, 레플리케이션 3 : 파티션 1개, 복제본 2개)

브로커 개수에 따라 레플리케이션 개수 제한(브로커 개수 3 → 레플리케이션 4가 될 수 없음)

원본 1개 파티션 - 리더 파티션

복제본 2개 파티션 - 팔로워 파티션

### 레플리케이션

파티션의 고가용성을 위해 사용

ex ) 브로커 3개, 파티션 1, 레플리케이션 1인 토픽 ⇒ 브로커 장애 발생 시 해당 파티션 복구 불가능

ex ) 브로커 3개, 파티션 1, 레플리케이션 2인 토픽 ⇒ 브로커 1개 죽어도 복제본, 팔로워 파티션이 있어서 복구 가능. 나머지 남은 팔로워 파티션이 리더 파티션 역할 승계

프로듀서가 토픽에 데이터 전달할 때, 전달 받는 주체가 ‘리더 파티션’

프로듀서에는 ack라는 상세 옵션 있음

ack 0 - 리더 파티션에 데이터 전송, 응답값 받지 않음 (정상 전송 됐는지, 나머지 파티션에 복제 됐는지 알 수 없음, 속도 빠르지만 데이터 유실 가능성 있음)

ack 1 - 리더 파티션에 데이터 전송, 제대로 전송 됐는지 응답값 받음, 나머지 파티션에 복제 되었는 지는 알 수 없음(ack 0 옵션처럼 데이터 유실 가능성 있음)

ack all - 팔로워 파티션에 잘 복제 되었는지 응답값 받음. 

레플리케이션이 많을 수록 가용성이 높을 수 있지만, 그만큼 브로커의 리소스 사용량도 늘어남. 카프카에 들어오는 데이터량과 retention date(저장시간)을 잘 생각해서 레플리케이션 개수 계획

3개 이상 브로커 사용 시 레플리케이션 3 추천

### 파티셔너

프로듀서가 데이터를 보내면 무조건 파티셔너를 통해서 브로커로 데이터가 전송됨

데이터를 토픽의 어떤 파티셔너에 넣을지 결정하는 역할

레코드에 포함된 메시지 키, 메시지 값에 따라서 파티션 위치 결정

메시지 키 있는 경우 - 파티셔너에 의해 특정한 해시값 생성, 이 해시값 기준으로 어느 파티션에 들어갈 지 정해짐

hash(”seoul”) = partition 0

hash(”busan”) = partition 1

hash(”ulsan”) = partition 0

고정된 파티션으로 들어감, 순서 지켜서 데이터 처리할 수 있는 장점

파티션 내부에서는 큐처럼 동작

메시지 키 없는 경우 - 라운드로빈으로 파티션에 들어감

UniformStickyPartitioner : 프로듀서에서 배치로 모을 수 있는 최대한의 레코드를 모아서 파티션으로 데이터 보냄

커스텀 파티셔너 생성할 수 있도록 Partitioner 인터페이스 제공

메시지 키, 값, 토픽 이름에 따라서 어느 파티션에 보낼 것인지 정할 수 있음

언제? VIP 고객을 위해 로직 처리를 빠르게 해야 할 때

AMQP 메시징 시스템 같은 곳에서 우선순위 큐 만드는 것과 비슷

## 카프카 랙

카프카 운영 모니터링 지표

카프카 파티션에 데이터가 들어가면, 각 데이터는 오프셋이라는 숫자가 붙음

파티션 1개인 토픽에 프로듀서가 데이터 넣으면 0부터 차례대로 번호 매겨짐

프로듀서가 데이터 넣는 속도가 컨슈머가 데이터 가져가는 속도보다 훨씬 빠르다면?

프로듀서가 넣은 데이터의 오프셋

컨슈머가 가져간 데이터의 오프셋

둘 간의 차이가 발생 ⇒ 컨슈머 랙

랙은 많을 수도 있고 적을 수도 있음, 주로 컨슈머의 상태에 대해서 볼 때 사용

랙 - 각 파티션의 오프셋 기준으로, 프로듀서가 넣은 데이터의 오프셋과 컨슈머가 가져가는 데이터 오프셋의 차이를 기반으로 함

토픽에 여러 파티션이 존재할 경우, 랙이 여러개 발생할 수 있음

컨슈머 그룹이 1개, 파티션이 2개인 토픽에서 데이터 가져간다면? 랙 2개 측정될 수 있음

랙이 여러 개 존재할 수 있을 때, 높은 숫자의 랙을 ‘records-lag-max’라고 부름

### 버로우

Kafka-client 라이브러리 사용해서 카프카 컨슈머 구현 가능

카프카컨슈머 객체를 통해 현재 랙 정보 가져올 수 있음

데이터를 엘라스틱서치, 인플럭스디비와 같은 저장소에 넣은 뒤 그라파나 대시보드 통해서 확인 가능

컨슈머 단에서 랙을 수집하는 것은 컨슈머 상태에 dependency가 걸림

컨슈머가 비정상적 종료 되면, 랙 정보를 더이상 보낼 수 없음. 랙 측정 불가능.

컨슈머가 개발될 때마다 컨슈머 랙 저장하는 로직 개발해야 함

1. 멀티 카프카 클러스터 지원
    
    대부분 2개 이상 운영
    
    버로우 어플리케이션 1개만 구동해도 모든 컨슈머 랙 모니터링 가능
    
2. 슬라이딩 윈도우를 통한 컨슈머의 status 확인 가능
    
    error, warning, ok 단계로 표현
    
    warning - 데이터 양이 일시적으로 많아져서 컨슈머 오프셋이 증가되고 있음
    
    error - 데이터 양이 많아지고 있는데 컨슈머가 가져가지 않으면 에러라고 정의
    
3. HTTP api 제공
    
    HTTP api로 조회 가능
    
    버로우는 다양한 추가 생태계 구축 가능해짐
